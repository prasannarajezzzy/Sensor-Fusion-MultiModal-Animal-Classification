{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Video Processing. Getting the audio and the video according to the defined format"
      ],
      "metadata": {
        "id": "-Xh66U19i3YP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cze_eNhf-pGE",
        "outputId": "b897c05e-ce3a-4294-d918-a66caba93357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_video(video_path, target_size=(224, 224), num_frames=16):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, target_size)\n",
        "        frame = frame.astype(np.float32) / 255.0\n",
        "        frames.append(frame)\n",
        "        if len(frames) == num_frames:\n",
        "            break\n",
        "    cap.release()\n",
        "    if len(frames) < num_frames:\n",
        "        frames += [frames[-1]] * (num_frames - len(frames))\n",
        "    return np.stack(frames, axis=0)\n",
        "\n",
        "video_path = \"test.mp4\"\n",
        "preprocessed_video = preprocess_video(video_path)\n",
        "print(preprocessed_video.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def get_audio(video_path, audio_path):\n",
        "    video_clip = VideoFileClip(video_path)\n",
        "\n",
        "    audio_clip = video_clip.audio\n",
        "\n",
        "    audio_clip.write_audiofile(audio_path)\n",
        "\n",
        "    video_clip.close()\n",
        "    audio_clip.close()\n",
        "\n",
        "audio_path = \"audio.wav\"\n",
        "get_audio(video_path, audio_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ppwV2b-fHwD",
        "outputId": "80952644-68a4-4fb3-884f-dcca3d77e116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_audio(audio_path, sample_rate=44100, n_mels=128, hop_length=512, duration=10):\n",
        "    y, sr = librosa.load(audio_path, sr=sample_rate, duration=duration, mono=True)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "    mel_spectrogram = (mel_spectrogram - np.min(mel_spectrogram)) / (np.max(mel_spectrogram) - np.min(mel_spectrogram))\n",
        "    return mel_spectrogram\n",
        "\n",
        "\n",
        "preprocessed_audio = preprocess_audio(audio_path)\n",
        "print(preprocessed_audio.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGYk983Neyx8",
        "outputId": "0c24a2a5-2fa0-4d4b-9b9c-91f911d109f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 577)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the CNN model with resnet50 base and extracting the features for audio and video images\n"
      ],
      "metadata": {
        "id": "P7ja-uAHi-s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "def create_video_feature_extractor(input_shape=(224, 224, 3), base_model='resnet50'):\n",
        "    if base_model == 'resnet50':\n",
        "        base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported base model\")\n",
        "\n",
        "    output = GlobalAveragePooling2D()(base.output)\n",
        "    model = Model(inputs=base.input, outputs=output)\n",
        "    return model\n",
        "\n",
        "video_feature_extractor = create_video_feature_extractor()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzdI9f6He0pr",
        "outputId": "953cd06c-b2a3-4db7-8af5-e27a0d3701ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "def create_audio_feature_extractor(input_shape=(128, 128, 1)):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "audio_feature_extractor = create_audio_feature_extractor()\n"
      ],
      "metadata": {
        "id": "sBq-rkR8e2V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining the 2 features"
      ],
      "metadata": {
        "id": "vohIfOAGjQzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def late_fusion(video_features, audio_features):\n",
        "    video_output = Dense(64, activation='relu')(video_features)\n",
        "    audio_output = Dense(64, activation='relu')(audio_features)\n",
        "    combined_features_late = Concatenate()([video_output, audio_output])\n",
        "    return combined_features_late\n",
        "\n",
        "combined_features_late = late_fusion(video_feature_extractor, audio_feature_extractor)\n"
      ],
      "metadata": {
        "id": "Y9o4DFd_e34j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def create_fusion_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_dim=input_dim),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "fusion_model = create_fusion_model(input_dim=combined_features.shape[1])\n",
        "fusion_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "fusion_model.summary()\n"
      ],
      "metadata": {
        "id": "KD3LEPx5e6LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics"
      ],
      "metadata": {
        "id": "L3pCF-zxjeUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = fusion_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = fusion_model.evaluate(X_val, y_val)\n",
        "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "zOnfyHAie8LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = fusion_model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "rD5_-WdEe-Pj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}